{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install Sastrawi\n",
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "042mSZBh4f6-",
        "outputId": "49c0ed6c-0d67-487c-a1ab-7154d1672751",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-play-scraper\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google_play_scraper import app\n",
        "from google_play_scraper import Sort, reviews\n",
        "\n",
        "result, continuation_token = reviews (\n",
        "   'com.jaklingkoindonesia.app',\n",
        "   lang='id',\n",
        "   country='id',\n",
        "   sort=Sort.NEWEST,\n",
        "   count=2000,\n",
        "   filter_score_with=None\n",
        ")\n",
        "dataset = pd.DataFrame(np.array(result), columns=['review'])\n",
        "dataset = dataset.join(pd.DataFrame(dataset.pop('review').tolist()))\n",
        "dataset = dataset.sort_values('at', ascending=True)\n",
        "\n",
        "filtered_dataset = dataset[\n",
        "   (dataset['at']>='2024-01-01') &\n",
        "   (dataset['at']<='2024-12-31')\n",
        "]\n",
        "\n",
        "filtered_dataset[['content', 'score', 'at']].to_csv('data_ulasan_mentah.csv', index=False)\n"
      ],
      "metadata": {
        "id": "CEDUuCjQdGFJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "file_data_mentah = \"data_ulasan.csv\"\n",
        "file_kamus_slang = \"kamus_slang.csv\"\n",
        "output_file_final_preprocessing = \"data_akhir.csv\"\n",
        "\n",
        "df = pd.read_csv(file_data_mentah, sep=';', on_bad_lines='skip')\n",
        "\n",
        "def clean_text_cleansing(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.strip()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "df['Content'] = df['Content'].astype(str).apply(clean_text_cleansing)\n",
        "df.drop_duplicates(subset=['Content'], inplace=True)\n",
        "df.dropna(subset=['Content'], inplace=True)\n",
        "\n",
        "def clean_text_casefolding(text):\n",
        "    if isinstance(text, str):\n",
        "        text = text.lower()\n",
        "        text = text.strip()\n",
        "        text = re.sub(r'[^\\w\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "    return text\n",
        "\n",
        "df['Content'] = df['Content'].astype(str).apply(clean_text_casefolding)\n",
        "\n",
        "df_slang = pd.read_csv(file_kamus_slang, sep=';')\n",
        "slang_dict = dict(zip(df_slang['slang'], df_slang['formal']))\n",
        "\n",
        "def convert_slangword(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    words = text.split()\n",
        "    normalized_words = [slang_dict.get(word.lower(), word) for word in words]\n",
        "    return ' '.join(normalized_words)\n",
        "\n",
        "df['Content'] = df['Content'].apply(convert_slangword)\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "def stem_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    return stemmer.stem(str(text))\n",
        "\n",
        "df['Content'] = df['Content'].apply(stem_text)\n",
        "\n",
        "list_stopwords = stopwords.words('indonesian')\n",
        "list_stopwords.append('nya')\n",
        "if 'tidak' in list_stopwords:\n",
        "    list_stopwords.remove('tidak')\n",
        "\n",
        "def remove_stopwords(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    words = word_tokenize(str(text))\n",
        "    filtered_words = [word for word in words if word.lower() not in list_stopwords]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "df['Content'] = df['Content'].astype(str).apply(remove_stopwords)\n",
        "def handle_negation(tokens):\n",
        "    new_tokens = []\n",
        "    i = 0\n",
        "    while i < len(tokens):\n",
        "        if tokens[i] == 'tidak' and i + 1 < len(tokens):\n",
        "            new_tokens.append(tokens[i] + '_' + tokens[i+1])\n",
        "            i += 2\n",
        "        else:\n",
        "            new_tokens.append(tokens[i])\n",
        "            i += 1\n",
        "    return new_tokens\n",
        "\n",
        "def tokenize_text_final(text):\n",
        "    if pd.isna(text):\n",
        "        return []\n",
        "    return word_tokenize(str(text))\n",
        "\n",
        "df['Content'] = df['Content'].apply(tokenize_text_final)\n",
        "\n",
        "df.to_csv(output_file_final_preprocessing, index=False)"
      ],
      "metadata": {
        "id": "Kbw1UFpR4irV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "import ast\n",
        "import os\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "file_data = \"data_akhir.csv\"\n",
        "df = pd.read_csv(file_data, sep=',', on_bad_lines='skip')\n",
        "\n",
        "df['Content'] = df['Content'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "label_mapping = {\n",
        "    'Negatif': 0,\n",
        "    'Positif': 1,\n",
        "}\n",
        "df['Sentiment_Numeric'] = df['Label'].map(label_mapping)\n",
        "\n",
        "df.dropna(subset=['Content', 'Sentiment_Numeric'], inplace=True)\n",
        "\n",
        "X_tokens = df['Content'].tolist()\n",
        "y_labels = df['Sentiment_Numeric'].tolist()\n",
        "\n",
        "print(f\"Jumlah dokumen (ulasan) untuk pelatihan Word2Vec: {len(X_tokens)}\")\n",
        "print(f\"Contoh ulasan tokenized pertama: {X_tokens[0]}\")\n",
        "print(f\"Contoh label sentimen pertama (numerik): {y_labels[0]}\")\n",
        "\n",
        "model_w2v_from_scratch = Word2Vec(\n",
        "    sentences=X_tokens,\n",
        "    vector_size=100,\n",
        "    window=5,\n",
        "    min_count=1,\n",
        "    sg=1,\n",
        "    workers=4\n",
        ")\n",
        "\n",
        "model_w2v_from_scratch.train(X_tokens, total_examples=model_w2v_from_scratch.corpus_count, epochs=40)\n",
        "model_w2v_from_scratch.save(\"word2vec_from_scratch_model.model\")\n",
        "joblib.dump(model_w2v_from_scratch, 'word2vec_model.pkl')\n",
        "\n",
        "\n",
        "def get_document_vector(word2vec_model, doc_tokens):\n",
        "    doc_vector = np.zeros(word2vec_model.wv.vector_size)\n",
        "    word_count = 0\n",
        "    for word in doc_tokens:\n",
        "        if word in word2vec_model.wv:\n",
        "            doc_vector += word2vec_model.wv[word]\n",
        "            word_count += 1\n",
        "\n",
        "    if word_count > 0:\n",
        "        return doc_vector / word_count\n",
        "    return doc_vector\n",
        "\n",
        "X_features_from_scratch = np.array([get_document_vector(model_w2v_from_scratch, tokens) for tokens in X_tokens])\n",
        "\n",
        "print(f\"Bentuk data fitur (X_features_from_scratch): {X_features_from_scratch.shape}\")\n",
        "print(f\"Contoh vektor ulasan pertama (5 elemen): {X_features_from_scratch[0][:5]}...\")"
      ],
      "metadata": {
        "id": "d6qAkjRPALv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "from collections import Counter\n",
        "\n",
        "print(f\"Distribusi kelas sebelum SMOTE: {Counter(y_labels)}\")\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "X_resampled, y_resampled = smote.fit_resample(X_features_from_scratch, y_labels)\n",
        "\n",
        "print(f\"Distribusi kelas setelah SMOTE: {Counter(y_resampled)}\")"
      ],
      "metadata": {
        "id": "8d8n3-PKiylm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled,\n",
        "    y_resampled,\n",
        "    test_size=0.2,\n",
        "    random_state=42,\n",
        "    stratify=y_resampled\n",
        ")\n",
        "\n",
        "svm_model = SVC(kernel='linear', random_state=42)\n",
        "svm_model.fit(X_train, y_train)\n",
        "joblib.dump(svm_model, 'svm_model.pkl')\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"\\n1. Akurasi Model: {accuracy:.4f}\")\n",
        "print(\"\\n2. Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Negatif (0)', 'Positif (1)']))\n"
      ],
      "metadata": {
        "id": "MSFH4I8qM0K8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "y_pred = svm_model.predict(X_test)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "class_labels = ['Negatif', 'Positif']\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.xlabel('Prediksi Kelas')\n",
        "plt.ylabel('Kelas Aktual')\n",
        "plt.title('Confusion Matrix Klasifikasi Sentimen')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0C6KficrOiBp",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from collections import defaultdict\n",
        "\n",
        "if not isinstance(y_resampled, np.ndarray):\n",
        "    y_resampled = np.array(y_resampled)\n",
        "n_splits = 10\n",
        "kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
        "\n",
        "svm_model_cv = SVC(kernel='linear', C=1, max_iter=150, random_state=42)\n",
        "results_per_fold = defaultdict(list)\n",
        "\n",
        "print(f\"Memulai Pengujian {n_splits}-Fold Cross Validation pada data yang sudah di-SMOTE...\")\n",
        "fold_count = 1\n",
        "for train_index, test_index in kf.split(X_resampled, y_resampled):\n",
        "    print(f\"--- Fold {fold_count} ---\")\n",
        "    X_train_fold, X_test_fold = X_resampled[train_index], X_resampled[test_index]\n",
        "    y_train_fold, y_test_fold = y_resampled[train_index], y_resampled[test_index]\n",
        "\n",
        "    svm_model_cv.fit(X_train_fold, y_train_fold)\n",
        "    y_pred_fold = svm_model_cv.predict(X_test_fold)\n",
        "\n",
        "    accuracy = accuracy_score(y_test_fold, y_pred_fold)\n",
        "    precision = precision_score(y_test_fold, y_pred_fold, average='binary', pos_label=1, zero_division=0)\n",
        "    recall = recall_score(y_test_fold, y_pred_fold, average='binary', pos_label=1, zero_division=0)\n",
        "    f1 = f1_score(y_test_fold, y_pred_fold, average='binary', pos_label=1, zero_division=0)\n",
        "\n",
        "    results_per_fold['Accuracy'].append(accuracy)\n",
        "    results_per_fold['Precision'].append(precision)\n",
        "    results_per_fold['Recall'].append(recall)\n",
        "    results_per_fold['F1-Score'].append(f1)\n",
        "\n",
        "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  Precision: {precision:.4f}\")\n",
        "    print(f\"  Recall: {recall:.4f}\")\n",
        "    print(f\"  F1-Score: {f1:.4f}\")\n",
        "\n",
        "    fold_count += 1\n",
        "\n",
        "print(f\"\\n--- Hasil Rata-rata dari {n_splits}-Fold Cross Validation ---\")\n",
        "print(\"N-Fold\\tAccuracy\\tPrecision\\tRecall\\tF1-Score\")\n",
        "for i in range(n_splits):\n",
        "    print(f\"{i+1}\\t{results_per_fold['Accuracy'][i]:.4f}\\t{results_per_fold['Precision'][i]:.4f}\\t{results_per_fold['Recall'][i]:.4f}\\t{results_per_fold['F1-Score'][i]:.4f}\")\n",
        "\n",
        "mean_accuracy = np.mean(results_per_fold['Accuracy'])\n",
        "mean_precision = np.mean(results_per_fold['Precision'])\n",
        "mean_recall = np.mean(results_per_fold['Recall'])\n",
        "mean_f1 = np.mean(results_per_fold['F1-Score'])\n",
        "\n",
        "print(f\"Mean\\t{mean_accuracy:.4f}\\t{mean_precision:.4f}\\t{mean_recall:.4f}\\t{mean_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "-o2BYUqh8dXI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "output_file_final_preprocessing = \"data_akhir.csv\"\n",
        "df_wordcloud = pd.read_csv(output_file_final_preprocessing, sep=',')\n",
        "\n",
        "df_wordcloud['Content'] = df_wordcloud['Content'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "positive_reviews = df_wordcloud[df_wordcloud['Label'] == 'Positif']['Content']\n",
        "\n",
        "all_positive_words = []\n",
        "for tokens in positive_reviews:\n",
        "    if isinstance(tokens, list) and tokens:\n",
        "        all_positive_words.extend(tokens)\n",
        "\n",
        "positive_word_counts = Counter(all_positive_words)\n",
        "\n",
        "positive_text_for_wc = \" \".join(all_positive_words)\n",
        "\n",
        "wordcloud_positive = WordCloud(width=800, height=800, background_color='black',\n",
        "                               colormap='Blues', min_font_size=10, max_words=200).generate(positive_text_for_wc)\n",
        "\n",
        "plt.figure(figsize=(10, 7), frameon=False)\n",
        "ax = plt.Axes(plt.gcf(), [0., 0., 1., 1.])\n",
        "ax.set_axis_off()\n",
        "plt.gcf().add_axes(ax)\n",
        "\n",
        "ax.imshow(wordcloud_positive, interpolation='bilinear')\n",
        "plt.tight_layout()\n",
        "plt.savefig('wordcloud_positif_noborder.png', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFrekuensi Kata Sentimen Positif (Top 20):\")\n",
        "for word, count in positive_word_counts.most_common(20):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "cu8Txd2rLNw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import ast\n",
        "import os\n",
        "from collections import Counter\n",
        "\n",
        "output_file_final_preprocessing = \"data_akhir.csv\"\n",
        "df_wordcloud = pd.read_csv(output_file_final_preprocessing, sep=',')\n",
        "df_wordcloud['Content'] = df_wordcloud['Content'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "negative_reviews = df_wordcloud[df_wordcloud['Label'] == 'Negatif']['Content']\n",
        "\n",
        "all_negative_words = []\n",
        "for tokens in negative_reviews:\n",
        "    if isinstance(tokens, list) and tokens:\n",
        "        all_negative_words.extend(tokens)\n",
        "\n",
        "negative_word_counts = Counter(all_negative_words)\n",
        "\n",
        "negative_text_for_wc = \" \".join(all_negative_words)\n",
        "\n",
        "wordcloud_negative = WordCloud(width=800, height=800, background_color='black',\n",
        "                               colormap='Reds', min_font_size=10, max_words=200).generate(negative_text_for_wc)\n",
        "\n",
        "plt.figure(figsize=(10, 7), frameon=False)\n",
        "ax = plt.Axes(plt.gcf(), [0., 0., 1., 1.])\n",
        "ax.set_axis_off()\n",
        "plt.gcf().add_axes(ax)\n",
        "\n",
        "ax.imshow(wordcloud_negative, interpolation='bilinear')\n",
        "plt.tight_layout()\n",
        "plt.savefig('wordcloud_negatif_noborder.png', bbox_inches='tight', pad_inches=0)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nFrekuensi Kata Sentimen Negatif (Top 20):\")\n",
        "for word, count in negative_word_counts.most_common(20):\n",
        "    print(f\"{word}: {count}\")"
      ],
      "metadata": {
        "id": "Qc7EjI7nLan-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}